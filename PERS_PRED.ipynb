{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "\n",
    "    #classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    #vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    #training features\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    #performance measures\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows and columns in train data:{}' .format(train.shape))\n",
    "print('Number of rows and columns in test data:{}' .format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there any missing data in any of the columns?\n",
    "print(\"TRAINING DATASET: \")\n",
    "train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST DATASET: \")\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many posts were written by each personality type?\n",
    "type_sum = train.groupby(['type']).count()\n",
    "type_sum.sort_values('posts', ascending=False, inplace=True)\n",
    "type_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total words were written by each personality type?\n",
    "train['word_count'] = train['posts'].apply(lambda x: len(str(x).split(\" \")))\n",
    "word_count = train.groupby('type').sum()\n",
    "word_count.sort_values('word_count', ascending=False, inplace=True)\n",
    "word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop word_count column\n",
    "train = train.drop(['word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary column for each of the 4 dimension types\n",
    "train['Mind'] = train['type'].map(lambda x: 'Extroverted' if x[0] == 'E' else 'Introverted')\n",
    "train['Energy'] = train['type'].map(lambda x: 'Intuitive' if x[1] == 'N' else 'Sensing')\n",
    "train['Nature'] = train['type'].map(lambda x: 'Thinking'  if x[2] == 'T' else 'Feeling')\n",
    "train['Tactics'] = train['type'].map(lambda x: 'Judging'  if x[3] == 'J' else 'Perceiving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mind\n",
    "# Countplot of the Introverted - Extroverted variable\n",
    "IEcolors = sns.xkcd_palette(['green', 'light green'])\n",
    "sns.set_palette(IEcolors)\n",
    "sns.countplot(x='Mind', data=train, order=['Introverted', 'Extroverted'])\n",
    "plt.ylim(0, 8000)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('Mind')\n",
    "plt.ylabel('Count of each Personality Type')\n",
    "plt.title('Introversion vs. Extroversion', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of the Intuitive - Sensing variable\n",
    "NScolors = sns.xkcd_palette(['blue', 'light blue'])\n",
    "sns.set_palette(NScolors)\n",
    "sns.countplot(x='Energy', data=train, order=['Intuitive', 'Sensing'])\n",
    "plt.title('Intuitive vs. Sensing', fontsize=14)\n",
    "plt.ylim(0, 8000)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of the Thinking - Feeling variable\n",
    "TFcolors = sns.xkcd_palette(['orange', 'pale orange'])\n",
    "sns.set_palette(TFcolors)\n",
    "sns.countplot(x='Nature', data=train, order=['Thinking', 'Feeling'])\n",
    "plt.title('Thinking vs. Feeling', fontsize=14)\n",
    "plt.ylim(0, 8000)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of Judging - Perceiving\n",
    "JPcolors = sns.xkcd_palette(['purple', 'lavender'])\n",
    "sns.set_palette(JPcolors)\n",
    "sns.countplot(x='Tactics', data=train, order=['Judging', 'Perceiving'])\n",
    "plt.title('Judging vs. Perceiving', fontsize=14)\n",
    "plt.ylim(0, 8000)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing delimeters\n",
    "def remove_delimiters (post):\n",
    "    new = post.replace('|||',' ')\n",
    "    return ' '.join(new.split())\n",
    "\n",
    "train['posts'] = train['posts'].apply(remove_delimiters)\n",
    "test['description'] = test['description'].apply(remove_delimiters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "\n",
    "#apply to train set\n",
    "train['posts'] = train['posts'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "#apply to test set\n",
    "test['description'] = test['description'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to lowercase\n",
    "train['posts'] = train['posts'].str.lower()\n",
    "\n",
    "test['description'] = test['description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "def remove_punctuation(post):\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "\n",
    "train['posts'] = train['posts'].apply(remove_punctuation)\n",
    "\n",
    "test['description'] = test['description'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "# Lematise posts\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train['lemma'] = [' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])for text in train['posts']]\n",
    "test['lemma'] = [' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])for text in test['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any stopwords in the text?\n",
    "#Check for stopwords train\n",
    "stop = stopwords.words('english')\n",
    "train['stopwords'] = train['lemma'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train[['lemma','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for stopwords test\n",
    "stop = stopwords.words('english')\n",
    "test['stopwords'] = test['lemma'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "test[['lemma','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "def remove_stop_words(word):\n",
    "    if word not in stop:\n",
    "        return word\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lemma_no_stop'] = [' '.join([remove_stop_words(word) for word in text.split(' ')])for text in test['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "#Create binary classes for each of the personality characteristics\n",
    "train['E'] = train['type'].apply(lambda x: x[0] == 'E').astype('int')\n",
    "train['N'] = train['type'].apply(lambda x: x[1] == 'N').astype('int')\n",
    "train['T'] = train['type'].apply(lambda x: x[2] == 'T').astype('int')\n",
    "train['J'] = train['type'].apply(lambda x: x[3] == 'J').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind_df = train[['lemma','E']]\n",
    "mind_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf * idf \n",
    "term frequency = no of rep of words in a sent / no of words in a sent\n",
    "inverse document frequency = no of sentences / no of sentences containing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_mind = TfidfVectorizer(lowercase=True, \n",
    "                            stop_words='english', \n",
    "                            max_features=250,\n",
    "                            min_df=4,\n",
    "                            max_df=0.5\n",
    "                           )\n",
    "vect_mind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_mind.fit(mind_df['lemma'])     #train \n",
    "X_count_mind = vect_mind.transform(mind_df['lemma']) \n",
    "X_count_mind   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_mind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_mind.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_count_mind\n",
    "y = mind_df['E']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size =0.4,\n",
    "                                                   random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function_log_loss(y_test, y_pred_test):\n",
    "    return log_loss(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_log_loss = make_scorer(scoring_function_log_loss, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_LogReg_model(X_train, y_train): \n",
    "    C_list = [0.001, 0.01, 0.1, 0.5, 0.75, 1, 5, 10, 25, 100]\n",
    "    penalty_list = ['l1','l2']\n",
    "\n",
    "    score = make_scorer(scoring_function_log_loss, greater_is_better = False)\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    \n",
    "    parameters = {'C':C_list,\n",
    "                  'penalty': penalty_list}\n",
    "    tune = GridSearchCV(logreg, parameters, scoring = score)\n",
    "    tune.fit(X_train,y_train)\n",
    "    \n",
    "    return tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mind_model = tune_LogReg_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mind_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind_model = LogisticRegression(C=best_mind_model.best_params_['C'], penalty = best_mind_model.best_params_['penalty'])\n",
    "mind_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = mind_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = mind_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred_test,labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred_test, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind_log_loss = cross_val_score(mind_model, X, y, scoring=score_log_loss,cv=4,)\n",
    "print('Log Loss %2f' %(-1 * mind_log_loss.mean()))\n",
    "\n",
    "mind_acc = cross_val_score(mind_model, X, y, scoring='accuracy',cv=4,)\n",
    "print('Accuracy %2f' %(mind_acc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
    "mind_model.fit(X_train, y_train)\n",
    "plot_confusion_matrix(mind_model,X_test,y_test,cmap=plt.cm.Purples,values_format = '.5g', display_labels = [\"Introverted\",\"Extroverted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy model\n",
    "energy_df = train[['lemma','N']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_energy = TfidfVectorizer(lowercase=True, \n",
    "                            stop_words='english', \n",
    "                            max_features=195,\n",
    "                            min_df=4,\n",
    "                            max_df=0.5\n",
    "                           )\n",
    "vect_energy.fit(energy_df['lemma'])\n",
    "X_count_energy = vect_energy.transform(energy_df['lemma'])\n",
    "\n",
    "X_count_energy.shape\n",
    "\n",
    "vect_energy.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_count_energy\n",
    "y = energy_df['N']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size =0.3,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_energy_model = tune_LogReg_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_energy_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_model = LogisticRegression(C=best_energy_model.best_params_['C'], penalty = best_energy_model.best_params_['penalty'])\n",
    "energy_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = energy_model.predict(X_train)\n",
    "\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = energy_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred_test, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred_test, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_log_loss = cross_val_score(energy_model, X, y, scoring=score_log_loss,cv=4)\n",
    "print('Log Loss %2f' %(-1 * energy_log_loss.mean()))\n",
    "\n",
    "energy_acc = cross_val_score(energy_model, X, y, scoring='accuracy',cv=4,)\n",
    "print('Accuracy %2f' %(energy_acc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
    "energy_model.fit(X_train, y_train)\n",
    "plot_confusion_matrix(energy_model,X_test,y_test,cmap=plt.cm.Reds,values_format = '.5g', display_labels = [\"Sensing\",\"Intuitive\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nature model\n",
    "nature_df = train[['lemma','T']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_nature = TfidfVectorizer(lowercase=True, \n",
    "                            stop_words='english', \n",
    "                            max_features=3900,\n",
    "                            min_df=4,\n",
    "                            max_df=0.5\n",
    "                           )\n",
    "vect_nature.fit(nature_df['lemma'])\n",
    "X_count_nature = vect_nature.transform(nature_df['lemma'])\n",
    "\n",
    "X_count_nature.shape\n",
    "\n",
    "vect_nature.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_count_nature\n",
    "y = nature_df['T']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size =0.3,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nature_model = tune_LogReg_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nature_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nature_model = LogisticRegression(C=best_nature_model.best_params_['C'], penalty = best_nature_model.best_params_['penalty'])\n",
    "nature_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = nature_model.predict(X_train)\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = nature_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred_test, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred_test, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nature_log_loss = cross_val_score(nature_model, X, y, scoring=score_log_loss,cv=4,)\n",
    "print('Log Loss %2f' %(-1 * nature_log_loss.mean()))\n",
    "\n",
    "nature_acc = cross_val_score(nature_model, X, y, scoring='accuracy',cv=4,)\n",
    "print('Accuracy %2f' %(nature_acc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
    "nature_model.fit(X_train, y_train)\n",
    "plot_confusion_matrix(nature_model,X_test,y_test,cmap=plt.cm.Blues,values_format = '.5g', display_labels = [\"Feeling\",\"Thinking\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tactics model\n",
    "tactics_df = train[['lemma','J']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tactics = TfidfVectorizer(lowercase=True, \n",
    "                            stop_words='english', \n",
    "                            max_features=260,\n",
    "                            min_df=4,\n",
    "                            max_df=0.5\n",
    "                           )\n",
    "vect_tactics.fit(tactics_df['lemma'])\n",
    "X_count_tactics = vect_tactics.transform(tactics_df['lemma'])\n",
    "\n",
    "X_count_tactics.shape\n",
    "\n",
    "vect_tactics.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_count_tactics\n",
    "y = tactics_df['J']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size =0.3,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tactics_model = tune_LogReg_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tactics_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tactics_model = LogisticRegression(C=best_tactics_model.best_params_['C'], penalty = best_tactics_model.best_params_['penalty'])\n",
    "tactics_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = tactics_model.predict(X_train)\n",
    "\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = tactics_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_train, y_pred_train, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "# confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred_test, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tactics_log_loss = cross_val_score(tactics_model, X, y, scoring=score_log_loss,cv=4,)\n",
    "print('Log Loss %2f' %(-1 * tactics_log_loss.mean()))\n",
    "\n",
    "tactics_acc = cross_val_score(tactics_model, X, y, scoring='accuracy',cv=4,)\n",
    "print('Accuracy %2f' %(tactics_acc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "tactics_model.fit(X_train, y_train)\n",
    "plot_confusion_matrix(tactics_model,X_test,y_test,cmap=plt.cm.Greens,values_format = '.5g', display_labels = [\"Perceiving\",\"Judging\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting mind\n",
    "pred_mind_count = vect_mind.transform(test['lemma_no_stop'])\n",
    "\n",
    "pred_mind_count.shape\n",
    "\n",
    "X = X_count_mind\n",
    "y = mind_df['E']\n",
    "\n",
    "final_mind_model = mind_model\n",
    "final_mind_model.fit(X, y)\n",
    "\n",
    "final_mind_predictions = final_mind_model.predict(pred_mind_count)\n",
    "\n",
    "test['E_pred'] = final_mind_predictions\n",
    "\n",
    "test.head()\n",
    "\n",
    "pred_mind_df = test[['username', 'E_pred']]\n",
    "\n",
    "pred_mind_df.head(10)\n",
    "\n",
    "pred_mind_df.columns\n",
    "\n",
    "pred_mind_df['E_pred'].value_counts().plot(kind = 'bar',color = ['dodgerblue','darkblue'])\n",
    "\n",
    "#pred_mind_df\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pred_mind_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting energy\n",
    "pred_energy_count = vect_energy.transform(test['lemma_no_stop'])\n",
    "\n",
    "pred_energy_count.shape\n",
    "\n",
    "X = X_count_energy\n",
    "y = energy_df['N']\n",
    "\n",
    "final_energy_model = energy_model\n",
    "final_energy_model.fit(X, y)\n",
    "\n",
    "final_energy_predictions = final_energy_model.predict(pred_energy_count)\n",
    "\n",
    "test['N_pred'] = final_energy_predictions\n",
    "\n",
    "pred_energy_df = test[['username', 'N_pred']]\n",
    "\n",
    "pred_energy_df['N_pred'].value_counts().plot(kind = 'bar', color = ['purple','violet'])\n",
    "plt.show()\n",
    "\n",
    "pred_energy_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting nature\n",
    "pred_nature_count = vect_nature.transform(test['lemma_no_stop'])\n",
    "\n",
    "pred_nature_count.shape\n",
    "\n",
    "X = X_count_nature\n",
    "y = nature_df['T']\n",
    "\n",
    "final_nature_model = nature_model\n",
    "final_nature_model.fit(X, y)\n",
    "\n",
    "final_nature_predictions = final_nature_model.predict(pred_nature_count)\n",
    "\n",
    "test['T_pred'] = final_nature_predictions\n",
    "\n",
    "pred_nature_df = test[['username', 'T_pred']]\n",
    "\n",
    "pred_nature_df['T_pred'].value_counts().plot(kind = 'bar', color = ['darkgreen','yellowgreen'])\n",
    "plt.show()\n",
    "\n",
    "pred_nature_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting tactics\n",
    "pred_tactics_count = vect_tactics.transform(test['lemma_no_stop'])\n",
    "\n",
    "pred_tactics_count.shape\n",
    "\n",
    "X = X_count_tactics\n",
    "y = tactics_df['J']\n",
    "\n",
    "final_tactics_model = tactics_model\n",
    "final_tactics_model.fit(X, y)\n",
    "\n",
    "final_tactics_predictions = final_tactics_model.predict(pred_tactics_count)\n",
    "\n",
    "test['J_pred'] = final_tactics_predictions\n",
    "\n",
    "pred_tactics_df = test[['username', 'J_pred']]\n",
    "\n",
    "pred_tactics_df['J_pred'].value_counts().plot(kind = 'bar', color = ['black','grey'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pred_tactics_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.merge(pred_mind_df[['username','E_pred']], pred_energy_df[['username','N_pred']], how ='inner', on ='username') \n",
    "my_submission = pd.merge(my_submission[['username','E_pred', 'N_pred']], pred_nature_df[['username','T_pred']], how ='inner', on ='username')\n",
    "my_submission = pd.merge(my_submission[['username','E_pred', 'N_pred','T_pred']], pred_tactics_df[['username','J_pred']], how ='inner', on ='username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.rename(columns={'username':'username',\n",
    "                            'E_pred':'mind',\n",
    "                            'N_pred': 'energy',\n",
    "                            'T_pred': 'nature',\n",
    "                            'J_pred': 'tactics'\n",
    "                             }, \n",
    "                 inplace=True)\n",
    "\n",
    "my_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for the predictions of each of the 4 chracteristics\n",
    "my_submission['Mind Pred'] = my_submission['mind'].map(lambda x: 'E' if x == 1 else 'I')\n",
    "my_submission['Energy Pred'] = my_submission['energy'].map(lambda x: 'N' if x == 1 else 'S')\n",
    "my_submission['Nature Pred'] = my_submission['nature'].map(lambda x: 'T' if x == 1 else 'F')\n",
    "my_submission['Tactics Pred'] = my_submission['tactics'].map(lambda x: 'J' if x == 1 else 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission['Personality Pred'] = my_submission['Mind Pred'] + my_submission['Energy Pred'] + my_submission['Nature Pred']+ my_submission['Tactics Pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission[['username','Personality Pred']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view posts count of each personality type\n",
    "# Countplot of the 16 personality types in the dataset\n",
    "dims1 = (15.0, 4.0)\n",
    "fig, ax = plt.subplots(figsize=dims1)\n",
    "cmrmap = sns.color_palette('CMRmap', 16)\n",
    "sns.set_palette(cmrmap)\n",
    "sns.countplot(x='type', data=train,\\\n",
    "              order=['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\\\n",
    "                    'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP'])\n",
    "plt.title('Distribution of Myers-Briggs Type Predictions in TRAIN DATASET', fontsize=16)\n",
    "plt.xlabel('Personality Type')\n",
    "plt.ylabel('Count of Posts')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#view posts count of each personality type\n",
    "# Countplot of the 16 personality types in the dataset\n",
    "dims1 = (15.0, 4.0)\n",
    "fig, ax = plt.subplots(figsize=dims1)\n",
    "cmrmap = sns.color_palette(\"CMRmap\", 16)\n",
    "sns.set_palette(cmrmap)\n",
    "sns.countplot(x='Personality Pred', data=my_submission,\\\n",
    "              order=['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\\\n",
    "                    'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP'])\n",
    "plt.title('Distribution of Myers-Briggs Type Predictions in TEST DATASET', fontsize=16)\n",
    "plt.xlabel('Personality Type')\n",
    "plt.ylabel('Count of Posts')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da310ddd9106c1158ceeb5935f084540406f49b91648600f05d04115e7fa6e43"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
